{% extends "base.html" %}

{% block title %}
How it works
{% endblock %}

{% block content %}

<style>
    .description_titles {
        color: #050404;
        font-family: sans-serif;
        text-align:center;
    }   

    .text {
        color: #181818;
        font-family: sans-serif;
        text-align:center;
    }

    .bloc1 {
        background-color:#DCD5D5;
        clear:both;
    }   

    .bloc2 {
        background-color:##FAF8F8;
        clear:both;
        display: block;
        margin-left: auto;
        margin-right: auto;
        width: 55%;
    } 

    .layer4 {
	border: solid 1px black;
	border-radius: 10%;
	/*display: block;
	margin-right: auto;
	margin-left: auto;*/
	width: 300px;
	height: 200px;
    }

   .layer5 {
	border: solid 1px black;
	border-radius: 10%;
	/*display: block;
	margin-right: auto;
	margin-left: auto;*/
	width: 650px;
	height: 400px;
    }

</style>

<div class='text'>

    <h1>How it works</h1>
    <br/>
        <p>
            Our colorizer algorithm is based on the <a href="https://phillipi.github.io/pix2pix/"> pix2pix </a> model
            from UC Berkeley.
        </p>
        <p>
            We adapted the architecture of the neural network for our project, coded the algorithm in TensorFlow and in
            PyTorch, wrapped it into a Flask API (TBC) before serving it as a microservice through Google Cloud (TBC).
        </p>
        <p>
            The model relies on a cGAN (conditional Generative Adversarial Network) to learn a mapping between the space
            of grayscale images to colored images.
        </p>
    <br/>



    <div class='bloc1'>
        <br/>
        <h1>Framing the problem</h1>
        <p>
            The problem consists in learning a mapping between the space of grayscale images (NxNx1 tensors) to the space
            of RGB images (NxNx3 tensors).
        </p>
        <p>
            <img class="layer4" title=".."
                 src="http://image.noelshack.com/fichiers/2018/43/1/1540242905-untitled-drawing-1.png"
                 align="center"/>
        </p>
        <br/>
        <br/>

        <p>
            To do so, we train a neural network to colorize a black and white image: Given a black and white image as
            input and a colored image as output, learn a mapping between them.
        </p>
        <p>
            To put it in simple words, it is trained to learn to "colorize inside the lines" and to give plausible
            colors (i.e. green is not a plausible color for a face, unless you live in a galaxy far far away...).
        </p>
        <p>
            In a way, it's a bit like teaching a child to draw!
        </p>
        <p>
            <img class="layer4" title=".."
                 src="http://image.noelshack.com/fichiers/2018/42/5/1539943464-44464933-610070852728922-544077605129682944-n.jpg"
                 align="center" />
        </p>
        <br/>
    </div>




    <div class='bloc2'>
        <br/>
        <h1> Conditional Generative Adversarial Networks</h1>
        <p>
            A simple way to train the neural net to colorize images would have been to use an objective function such as a
            L1 or L2 loss and compare the real colored image to the fake one generated by the network.
            However this approach does not give the expected result and has the major drawback of averaging plausible colors,
            thus giving a gray blurry image.
        </p>
        <p>
            To overcome this issue, we used a cGAN to learn a mapping between grayscale and RGB images as well as a custom
            loss function adapted to our problem. To put it in simple words, a GAN (a cGAN is just an extension of GANs) is
            a deep learning architecture where 2 networks compete one against the other: a Generator generates an image and
            tries to fool the Discriminator that learns to detect whether a colored image is real or generated. The system
            would reach an equilibrium.
        </p>
        <p>
            In our case, we use a conditional GAN where the Generator and the Discriminator are given an input black and
            white image, and learn either the mapping and the discrimination function conditionally to this black and white
            image.
        </p>

        <br/>

        <div id="attachment_965" class="wp-caption aligncenter">
                <img class="layer5" title=".."
                src="http://image.noelshack.com/fichiers/2018/42/5/1539941385-1539872741-screen-shot-2018-10-18-at-15-35-16.png"
                width="800" height="400" align="center" />
        </div>

        <br>

        <p> The Generator's job is to learn how to produce real looking colorized versions so that the Discriminator can't
            distinguish between the generated image and the real one. Whereas the Discriminator needs to keep up with the
            Generator in order not to be fooled by its improvements.
            Eventually, the Generator learns well the underlying distribution of color images data and becomes really good
            at generating colorized versions of black and white images.
        </p>
        <br/>
    </div>



    <div class='bloc1'>
        <br/>
        <h1>Our Data</h1>
        <p>
            We used the publicly-available dataset <a href="http://vis-www.cs.umass.edu/lfw/">Labeled Faces in the Wild</a>
            which contains more than 13,000 images of faces.
        </p>
        <p>
            We did not investigate if the data was biased (in terms of age, ethnicity or gender representation) though,
            but it is at least a good basis for starting this project.
        </p>
        <img title=".."
                src="https://www-tc.pbs.org/wgbh/nova/media/images/lfw-sample.width-800.jpg"
                align="center" width="900" height="450"/>
        <br/>
        <br/>
        <br/>
    </div>



    <div class='bloc2'>
        <br/>
        <h1>Our Model</h1>
        <br/>
        <p>
            <img class="layer4" title=".."
                    src="http://image.noelshack.com/fichiers/2018/42/5/1539943464-44464933-610070852728922-544077605129682944-n.jpg"
                    align="center" />
        </p>
        <br/>
        <p>
            Our model was trained on a library of 13,000 human portraits. As suggested by the pix2pix paper, the objective
            function was the classical Binary Cross Entropy loss used to train a cGAN to which we added a L1 loss so that
            the Generator would give sharper images. The two networks were trained using an Adam optimizer.
        </p>
        <br/>
        <img src="http://image.noelshack.com/fichiers/2018/43/2/1540283100-latex-3c0de927fdc7b9a0b39e481294045d4f.png" width="900"/>
        <br/>
        <br/>
        <p>
            We reproduced the paper's neural networks architectures. The generator is a
            <a href="https://arxiv.org/pdf/1505.04597.pdf"> U-net </a>, which is a modified version of the auto-encoder.
            One of its applications is segmentation in medical images, here it helps segment the different parts of faces
            which forces the algorithm to "colorize inside the lines". The discriminator is a classical convolutional
            discriminator.
        </p>
    </div>
    <br/>

        TODO: add image of the architectures

    <!--
    <div id="attachment_965" class="wp-caption aligncenter">
                <img class=" wp-image-965 " title=".."
                src="http://image.noelshack.com/fichiers/2018/42/5/1539943026-screen-shot-2018-10-19-at-11-56-30.png"
                width="900" height="550" align="center" />

    </div>
    <br/>
    <div id="attachment_965" class="wp-caption aligncenter">

                <img class=" wp-image-965 " title=".."
                src="http://image.noelshack.com/fichiers/2018/42/5/1539943018-screen-shot-2018-10-19-at-11-56-05.png"
                width="700" height="500" align="center" />

    </div>
    -->

    <div class='bloc1'>
        <br/>
        <h1>Results</h1>
        <h2>Training phase</h2>
        <br/>
        <p>
        We initially trained our model on a small batch of 15 political leaders to ensure that our network had the right
        architecture to learn and overfit a small dataset.
        </p>
        <p>
        This is shown on the following picture where the images on the right
        are the original images and the ones on the left are the images generated by the generator.
        </p>
        <br/>
        <br/>
        <p>
            <img class=" wp-image-965 " title=".."
                    src="http://image.noelshack.com/fichiers/2018/42/5/1539943340-leaders.jpg"
                    width="600" height="400" align="center" />
        </p>

        <br/>
        <br/>
        <p>
            We obtained the following loss (on the training set):
        </p>
        <p>
            At the beginning the generator has a very poor performance since it generates images at random, but gets better
            with time.
        </p>
        <p>
            As the generator improves, the discriminator's performance decreases (the discriminator gets fooled by the
            generator).
        </p>
        <p>
            The model eventually reaches an equilibrium.
        </p>
        <p>
            <img title=".."
                    src="http://image.noelshack.com/fichiers/2018/44/2/1540897619-loss.png"
                    align="center" width="700" height="350"/>
        </p>


        <h2>Inference phase</h2>
        <br/>
        <div id="attachment_965" class="wp-caption aligncenter">
            <img class=" wp-image-965 " title="Le protocole HTTP, en gros"
                 src="http://image.noelshack.com/fichiers/2018/42/5/1539947871-untitled-drawing.jpg?fbclid=IwAR2tVWmpw4aldOO8fPMIGhp_-D8rSaK5eHotbZfz0xLP5Dpp7EKnu0-32Co"
                 width="500" height="150" align="center" />
        </div>
        <br/>
        <br/>
        <p>
            On the left of the picture you can see an initial photo of Heidi Klum and Seal.
        </p>
        <p>
            We turned it into a B&W image we passed it to our colorizer model and obtained a coloured image afterwards.
        </p>
        <br/>

        <h2>Other examples</h2>
        <br>
    </div>
</div>

{% endblock %}